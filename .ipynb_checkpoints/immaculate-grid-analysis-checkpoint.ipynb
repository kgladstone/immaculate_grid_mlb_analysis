{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80e1bd21-21ed-439b-af8e-7e8003f7f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from refresh_db import ImmaculateGridUtils\n",
    "import numpy as np\n",
    "from copy import deepcopy \n",
    "from typing import Dict, List, Tuple, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07674256-9213-4a3a-9759-f1f502b21395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was trimmed to 2244 from original size of 2275, handling instances of multiple entries per person per grid\n"
     ]
    }
   ],
   "source": [
    "INPUT_GRID_RESULTS_FILE_PATH = './results.csv'\n",
    "INPUT_PROMPT_DATA_PATH = './prompts.csv'\n",
    "COLOR_MAP = {\"Sam\": \"red\", \"Keith\": \"blue\", \"Will\": \"purple\", \"Rachel\": \"green\", \"Cliff\": \"orange\"}\n",
    "PDF_FILENAME = \"./immaculate_grid_report.pdf\"\n",
    "TEAM_LIST = [\"Cubs\", \"Cardinals\", \"Brewers\", \"Reds\", \"Pirates\", \"Nationals\", \"Mets\", \"Marlins\", \"Phillies\", \"Braves\", \"Dodgers\", \"Diamondbacks\", \"Rockies\", \"Giants\", \"Padres\", \"Royals\", \"White Sox\", \"Twins\", \"Guardians\", \"Tigers\", \"Red Sox\", \"Yankees\", \"Blue Jays\", \"Rays\", \"Orioles\", \"Angels\", \"Athletics\", \"Astros\", \"Mariners\", \"Rangers\"]\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "def _to_percent(y, position):\n",
    "    \"\"\"Convert a decimal to a percentage string.\"\"\"\n",
    "    return f\"{100 * y:.0f}%\"\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# (0) Read data\n",
    "\n",
    "# Expand user directory and open the csv file\n",
    "data = pd.read_csv(INPUT_GRID_RESULTS_FILE_PATH, index_col=False)\n",
    "num_rows = len(data)\n",
    "\n",
    "# Keeping only the highest score for each grid_number and name combination\n",
    "data = data.loc[data.groupby(['grid_number', 'name'])['score'].idxmax()]\n",
    "num_rows_post = len(data)\n",
    "\n",
    "print(\"Data was trimmed to {} from original size of {}, handling instances of multiple entries per person per grid\".format(num_rows_post, num_rows))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# (1) Transform into base model format\n",
    "texts = ImmaculateGridUtils.df_to_immaculate_grid_objs(data)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# (2) Make a dictionary called \"reversed_dict\" that is the reverse of texts\n",
    "\n",
    "# Initialize the reversed dictionary\n",
    "reversed_dict = {}\n",
    "\n",
    "# Iterate over each name and the list of grid objects\n",
    "for name, grid_objects in texts.items():\n",
    "    for grid_obj in grid_objects:\n",
    "        # Extract the grid number from the text field of the object\n",
    "        grid_number = grid_obj.grid_number\n",
    "\n",
    "        if grid_number is not None:\n",
    "            # Set up the reversed dictionary so that the grid number points to the player and their result\n",
    "            reversed_dict.setdefault(grid_number, {})[name] = grid_obj\n",
    "\n",
    "### --- Make a dataframe for score, correctness, and average_score_of_correct\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# (3) Store in a Pandas dataframe\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "rows = []\n",
    "\n",
    "# Loop through the texts to gather data\n",
    "for person, grid_objects in texts.items():\n",
    "    for grid_obj in grid_objects:\n",
    "        # Extract the grid number from the text field of the object\n",
    "        grid_number = grid_obj.grid_number\n",
    "        \n",
    "        if grid_number is not None:\n",
    "            # Calculate average rarity\n",
    "            total_score_of_correct_squares = grid_obj.score - (100 * (9 - grid_obj.correct))\n",
    "            if grid_obj.correct == 0:\n",
    "                average_score_of_correct_squares = 100\n",
    "            else:\n",
    "                average_score_of_correct_squares = total_score_of_correct_squares / grid_obj.correct\n",
    "            \n",
    "            # Produce dataset\n",
    "            row = {\n",
    "                \"grid_number\": grid_number,  # Use a colon here\n",
    "                \"name\": grid_obj.name,\n",
    "                \"correct\": grid_obj.correct,\n",
    "                \"score\": grid_obj.score,\n",
    "                \"average_score_of_correct\": average_score_of_correct_squares,\n",
    "                \"date\": grid_obj.date,\n",
    "                \"matrix\": grid_obj.matrix\n",
    "            }\n",
    "            rows.append(row)  # Append the row to the list\n",
    "\n",
    "# Create the DataFrame from the list of rows\n",
    "analysis_df = pd.DataFrame(rows)\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "analysis_df['date'] = pd.to_datetime(analysis_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "effd179a-a308-449b-af82-a784485e9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate smoothed metrics (score, correct, average_score_of_correct) from analysis_df\n",
    "def calculate_smoothed_metrics(analysis_df: pd.DataFrame, smoothness: int) -> pd.DataFrame:\n",
    "    \"\"\"Generate a DataFrame of smoothed scores, correct values, and average scores over time.\"\"\"\n",
    "    metric_table = []\n",
    "\n",
    "    # Group the data by 'name' to process each person individually\n",
    "    grouped = analysis_df.groupby('name')\n",
    "\n",
    "    # Loop through each person\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(by='date')  # Sort by date to ensure time-based smoothing\n",
    "        scores = group['score'].tolist()  # Extract scores\n",
    "        corrects = group['correct'].tolist()  # Extract correct values\n",
    "        avg_scores = group['average_score_of_correct'].tolist()  # Extract average score of correct\n",
    "        dates = group['date'].tolist()  # Extract dates\n",
    "\n",
    "        # Apply smoothing with the specified window size\n",
    "        for i in range(smoothness, len(scores)):\n",
    "            # Extract windows of each metric\n",
    "            score_window = scores[i - smoothness:i]\n",
    "            correct_window = corrects[i - smoothness:i]\n",
    "            avg_score_window = avg_scores[i - smoothness:i]\n",
    "            \n",
    "            # Calculate smoothed values for each metric\n",
    "            valid_scores = [score for score in score_window if score is not None]\n",
    "            valid_corrects = [correct for correct in correct_window if correct is not None]\n",
    "            valid_avg_scores = [avg_score for avg_score in avg_score_window if avg_score is not None]\n",
    "\n",
    "            smoothed_score = sum(valid_scores) / len(valid_scores) if valid_scores else None\n",
    "            smoothed_correct = sum(valid_corrects) / len(valid_corrects) if valid_corrects else None\n",
    "            smoothed_avg_score = sum(valid_avg_scores) / len(valid_avg_scores) if valid_avg_scores else None\n",
    "            smoothed_date = dates[i] if i < len(dates) else None\n",
    "\n",
    "            # Only add rows where there are valid smoothed values\n",
    "            if smoothed_score is not None and smoothed_correct is not None and smoothed_avg_score is not None:\n",
    "                metric_table.append({\n",
    "                    'name': name,\n",
    "                    'grid_number': i,  # Could be i, or a corresponding column like group['grid_number']\n",
    "                    'smoothed_score': smoothed_score,\n",
    "                    'smoothed_correct': smoothed_correct,\n",
    "                    'smoothed_avg_score': smoothed_avg_score,\n",
    "                    'date': smoothed_date\n",
    "                })\n",
    "\n",
    "    # Create a DataFrame from the smoothed data\n",
    "    return pd.DataFrame(metric_table, columns=[\"name\", \"grid_number\", \"smoothed_score\", \"smoothed_correct\", \"smoothed_avg_score\", \"date\"]).dropna()\n",
    "\n",
    "# Function to plot smoothed metrics using the smoothed DataFrame\n",
    "def plot_smoothed_metrics(smoothed_df: pd.DataFrame, metric: str, title: str, ylabel: str) -> None:\n",
    "    \"\"\"Plot the smoothed metrics (score, correct, or average score) over time.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot smoothed metrics for each person\n",
    "    for name in smoothed_df['name'].unique():\n",
    "        person_data = smoothed_df[smoothed_df['name'] == name]\n",
    "        \n",
    "        # Plot line with proper date formatting for the selected metric\n",
    "        plt.plot(person_data['date'], person_data[metric], label=name, color=COLOR_MAP.get(name, 'blue'))\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Adjust x-axis date formatting and tick placement\n",
    "    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b %Y'))  # Format to month and year\n",
    "    plt.gca().xaxis.set_major_locator(plt.MaxNLocator(nbins=10))  # Limit number of x-ticks to avoid clutter\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better display\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------\n",
    "def calculate_win_rates(reversed_dict, criterion):\n",
    "    \"\"\"\n",
    "    Calculate win rates based on a given criterion.\n",
    "\n",
    "    Args:\n",
    "        reversed_dict (dict): The games data.\n",
    "        criterion (str): The criterion to calculate win rates (\"overall\", \"correctness\", \"scores\", \"last_rate\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of win rates for each person.\n",
    "    \"\"\"\n",
    "    wins = {person: 0 for person in texts}\n",
    "    for game in reversed_dict.values():\n",
    "        if criterion == \"overall\":\n",
    "            best = max((game[person].correct * 1000) + (1000 - game[person].score) for person in game)\n",
    "            for person in game:\n",
    "                effective_score = (game[person].correct * 1000) + (1000 - game[person].score)\n",
    "                if effective_score == best:\n",
    "                    wins[person] += 1\n",
    "        elif criterion == \"correctness\":\n",
    "            best = max(game[person].correct for person in game)\n",
    "            for person in game:\n",
    "                if game[person].correct == best:\n",
    "                    wins[person] += 1\n",
    "        elif criterion == \"scores\":\n",
    "            best = min(game[person].score for person in game)\n",
    "            for person in game:\n",
    "                if game[person].score == best:\n",
    "                    wins[person] += 1\n",
    "        elif criterion == \"last_rate\":\n",
    "            best = min((game[person].correct * 1000) + (1000 - game[person].score) for person in game)\n",
    "            for person in game:\n",
    "                effective_score = (game[person].correct * 1000) + (1000 - game[person].score)\n",
    "                if effective_score == best:\n",
    "                    wins[person] += 1\n",
    "\n",
    "    for person in wins:\n",
    "        wins[person] /= len(reversed_dict.values())\n",
    "\n",
    "    return wins\n",
    "\n",
    "def plot_win_rates(reversed_dict):\n",
    "    \"\"\"Plot win rates based on various criteria.\"\"\"\n",
    "    # Set a larger figure size to widen the graphs\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    criteria = [\"overall\", \"correctness\", \"scores\", \"last_rate\"]\n",
    "    titles = [\"Win Rates (Overall)\", \"Win Rates (Correctness Only)\", \"Win Rates (Scores Only)\", \"Last Rate (Overall)\"]\n",
    "\n",
    "    for ax, criterion, title in zip(axs.flat, criteria, titles):\n",
    "        wins = calculate_win_rates(reversed_dict, criterion)\n",
    "        ax.bar([person for person in wins], wins.values(), color=[COLOR_MAP[person] for person in wins])\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticks([i / 5 for i in range(6)])\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(_to_percent))\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "# Graph number of immaculates\n",
    "def make_fig_1(texts, COLOR_MAP):\n",
    "    counts = []\n",
    "    for person in texts:\n",
    "        data = [(1 if obj.correct == 9 else 0) for obj in texts[person]]\n",
    "        counts.append(sum(data))\n",
    "    plt.bar([person for person in texts], counts, color=[COLOR_MAP[person] for person in texts])\n",
    "    plt.title(\"Number of Immaculates\")\n",
    "    plt.show()\n",
    "    \n",
    "# Graph distributions\n",
    "def make_fig_2(texts, COLOR_MAP):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(10, 10))  # Create a 3x2 grid for 5 plots\n",
    "    top_bar = 130\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i, person in enumerate(texts):\n",
    "        distribution = [0 for _ in range(0, 10)]\n",
    "        for row in texts[person]:\n",
    "            distribution[row.correct] += 1\n",
    "        \n",
    "        # Plotting the distribution for each person\n",
    "        axs[i].bar(range(0, 10), distribution, color=COLOR_MAP[person])\n",
    "        axs[i].set_xticks(range(0, 10))\n",
    "        axs[i].set_title(person)\n",
    "        axs[i].set_ylim(0, 1.2*top_bar)\n",
    "    \n",
    "    # Hide the last subplot if it is not used\n",
    "    if len(texts) < 6:\n",
    "        axs[5].set_visible(False)\n",
    "    \n",
    "    fig.suptitle(\"Correctness Distribution\")\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# Graph average correct\n",
    "def make_fig_3(analysis_df, COLOR_MAP):\n",
    "    title = \"Average Correct\"\n",
    "    analysis_summary = analysis_df.groupby('name')['correct'].mean().reset_index()\n",
    "    \n",
    "    plt.bar(\n",
    "        analysis_summary.name, \n",
    "        analysis_summary.correct, \n",
    "        color=[COLOR_MAP[person] for person in analysis_summary.name])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Graph average score\n",
    "def make_fig_4(analysis_df, COLOR_MAP):\n",
    "    title = \"Average Score\"\n",
    "    analysis_summary = analysis_df.groupby('name')['score'].mean().reset_index()\n",
    "    \n",
    "    plt.bar(\n",
    "        analysis_summary.name, \n",
    "        analysis_summary.score, \n",
    "        color=[COLOR_MAP[person] for person in analysis_summary.name])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Graph average rarity of correct square\n",
    "def make_fig_5(analysis_df, COLOR_MAP):\n",
    "    title = \"Average Rarity of Correct Square\"\n",
    "    analysis_summary = analysis_df.groupby('name')['average_score_of_correct'].mean().reset_index()\n",
    "    \n",
    "    plt.bar(\n",
    "        analysis_summary.name, \n",
    "        analysis_summary.average_score_of_correct, \n",
    "        color=[COLOR_MAP[person] for person in analysis_summary.name])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot each metric separately\n",
    "def make_fig_6(smoothed_metrics_df):\n",
    "    plot_smoothed_metrics(smoothed_metrics_df, 'smoothed_score', \"Smoothed Scores Over Time\", \"Smoothed Score\")\n",
    "\n",
    "def make_fig_7(smoothed_metrics_df):\n",
    "    plot_smoothed_metrics(smoothed_metrics_df, 'smoothed_correct', \"Smoothed Correct Over Time\", \"Smoothed Correct\")\n",
    "\n",
    "def make_fig_8(smoothed_metrics_df):\n",
    "    plot_smoothed_metrics(smoothed_metrics_df, 'smoothed_avg_score', \"Smoothed Avg Score of Correct Over Time\", \"Smoothed Avg Score of Correct\")\n",
    "\n",
    "def make_fig_9(reversed_dict):\n",
    "    plot_win_rates(reversed_dict)\n",
    "\n",
    "# Function to format each record with proper alignment\n",
    "def format_record(rank, name, score, date, game_id, name_width=7, score_width=2, date_width=10, game_id_width=4):\n",
    "    formatted_rank = f'{rank:<2}'\n",
    "    formatted_name = f'{name:<{name_width}}'\n",
    "    formatted_score = f'{str(score):<{score_width}}'\n",
    "    formatted_date = f'{str(date):<{date_width}}'\n",
    "    formatted_game_id = f'{str(game_id):<{game_id_width}}'\n",
    "    \n",
    "    return f'{formatted_rank} | {formatted_name} | {formatted_score} | {formatted_date} | {formatted_game_id}'\n",
    "\n",
    "\n",
    "def make_fig_10(texts, fig_title='Best and Worst Scores (All Time)'):\n",
    "    \"\"\"\n",
    "    Creates a summary page in the PDF with the best and worst scores.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: Data used for creating score records.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare score records\n",
    "    score_records = []\n",
    "    for person, games in texts.items():\n",
    "        for game in games:\n",
    "            grid_id = game.grid_number\n",
    "            score_records.append((person, game.score, game.date, grid_id))\n",
    "    \n",
    "    # Sort records by score\n",
    "    sorted_records = sorted(score_records, key=lambda x: x[1])\n",
    "    \n",
    "    # Extract best and worst scores\n",
    "    best_records = sorted_records[:25]\n",
    "    worst_records = sorted_records[-25:][::-1]\n",
    "    \n",
    "    # Create a summary page with results\n",
    "    plt.figure(figsize=(8.5, 11))\n",
    "    plt.text(0.5, 0.97, fig_title, fontsize=25, ha='center', va='top', fontweight='bold')\n",
    "        \n",
    "    # Display best scores in a structured format with dynamic spacing\n",
    "    plt.text(0, 0.85, 'Best Scores:', fontsize=16, ha='left', va='top', fontweight='bold')\n",
    "    plt.text(0, 0.80, 'Rank | Name        | Score  | Date       | Game ID', fontsize=10, ha='left', va='top')\n",
    "    \n",
    "    for i, (name, score, date, game_id) in enumerate(best_records):\n",
    "        record_text = format_record(i + 1, name, score, date, game_id)\n",
    "        plt.text(0, 0.75 - i * 0.025, record_text, fontsize=10, ha='left', va='top', fontfamily='monospace')\n",
    "    \n",
    "    # Worst Scores Section\n",
    "    plt.text(0.6, 0.85, 'Worst Scores:', fontsize=16, ha='left', va='top', fontweight='bold')\n",
    "    plt.text(0.6, 0.80, 'Rank | Name        | Score  | Date       | Game ID', fontsize=10, ha='left', va='top')\n",
    "    \n",
    "    # Display worst scores in a structured format with dynamic spacing\n",
    "    for i, (name, score, date, game_id) in enumerate(worst_records):\n",
    "        record_text = format_record(i + 1, name, score, date, game_id)\n",
    "        plt.text(0.6, 0.75 - i * 0.025, record_text, fontsize=10, ha='left', va='top', fontfamily='monospace')\n",
    "\n",
    "    \n",
    "    plt.axis('off')  # Hide axes for the results page\n",
    "    plt.show()  # Close the figure\n",
    "\n",
    "def make_fig_11(texts):\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Calculate the date 30 days ago\n",
    "    thirty_days_ago = str(current_date - timedelta(days=30))\n",
    "    \n",
    "    # Filter the texts dictionary\n",
    "    filtered_texts = {\n",
    "        person: [game for game in games if game.date >= thirty_days_ago]\n",
    "        for person, games in texts.items()\n",
    "    }\n",
    "\n",
    "    fig_title = 'Best and Worst Scores (Last 30 Days)'\n",
    "    make_fig_10(filtered_texts, fig_title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86ba12e4-f1ba-4481-b3f8-e46dbb114caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Everything below this line incorporates prompt data\n",
    "#### TODO: Incorporate into PDF\n",
    "def read_prompt_data(filepath):\n",
    "    with open(os.path.expanduser(filepath)) as f:\n",
    "        prompt_df = pd.read_csv(f, header=None)\n",
    "    prompt_df.columns = [\"game_id\", \"00\", \"01\", \"02\", \"10\", \"11\", \"12\", \"20\", \"21\", \"22\"]\n",
    "    prompt_df = prompt_df.iloc[1:]\n",
    "    \n",
    "    new_rows = []\n",
    "    for i, row in prompt_df.iterrows():\n",
    "        new_row = {}\n",
    "        for col, val in row.items():\n",
    "            for char in [\"(\", \"'\", \")\"]:\n",
    "                val = val.replace(char, \"\")\n",
    "            new_row[col] = val.replace(\", \", \" + \")\n",
    "        new_rows.append(new_row)\n",
    "             \n",
    "    prompt_df = pd.DataFrame(new_rows)\n",
    "    prompt_df['game_id'] = prompt_df['game_id'].astype(int)\n",
    "\n",
    "    return prompt_df\n",
    "\n",
    "def category_is_team(category):\n",
    "    for team in TEAM_LIST:\n",
    "        if team in category:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_team_from_category(category):\n",
    "    for team in TEAM_LIST:\n",
    "        if team in category:\n",
    "            return team\n",
    "    return \"\"\n",
    "\n",
    "def get_categories_from_prompt(prompt):\n",
    "    parts = prompt.split(\" + \")\n",
    "    return parts[0].strip(), parts[1].strip()\n",
    "\n",
    "# Build up category data structure\n",
    "def build_category_structure(prompt_df):\n",
    "    categories = set()\n",
    "    for person, games in texts.items():\n",
    "        for game in games:\n",
    "            id = game.grid_number\n",
    "            prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "            if len(prompt_rows) != 1:\n",
    "                continue\n",
    "            prompts = prompt_rows.iloc[0][1:]\n",
    "            for prompt in prompts:\n",
    "                part_one, part_two = get_categories_from_prompt(prompt)\n",
    "                categories.add(part_one)\n",
    "                categories.add(part_two)\n",
    "    return categories\n",
    "\n",
    "# Initialize person<>category data structure\n",
    "# Dictionary where the key is the person name\n",
    "# The value is a list of sub-dictionaries\n",
    "# Each subdictionary's key is a category, and their value is a list of length=2\n",
    "# The first element in the list is the number instances of the person getting the value in the category correct\n",
    "# The second element in the list is the number of opportunities that the person had to answer for that category\n",
    "def build_person_category_structure(texts, prompt_df, categories):\n",
    "    person_to_category = {}\n",
    "    for person, _ in texts.items():\n",
    "        person_to_category[person] = {cat: [0, 0] for cat in categories}\n",
    "    \n",
    "    for person, games in texts.items():\n",
    "        for game in games:\n",
    "            id = game.grid_number\n",
    "            prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "            if len(prompt_rows) != 1:\n",
    "                continue\n",
    "            prompts = prompt_rows.iloc[0][1:]\n",
    "    \n",
    "            matrix = game.matrix\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "                    if matrix[i][j]:\n",
    "                        person_to_category[person][part_one][0] += 1\n",
    "                        person_to_category[person][part_two][0] += 1\n",
    "                    person_to_category[person][part_one][1] += 1\n",
    "                    person_to_category[person][part_two][1] += 1\n",
    "    return person_to_category\n",
    "\n",
    "def get_category_clearing_threshold(categories, person_to_category, threshold=25):\n",
    "    categories_to_count = {}\n",
    "    for category in categories:\n",
    "        categories_to_count[category] = []\n",
    "    for _, value in person_to_category.items():\n",
    "        for category, (correct, total) in value.items():\n",
    "            categories_to_count[category].append(total)\n",
    "    categories_clearing_threshold = [cat for cat in filter(lambda x: sum(categories_to_count[x]) / len(categories_to_count[x]) > threshold, categories_to_count)]\n",
    "    return categories_clearing_threshold\n",
    "\n",
    "def get_person_to_type(texts, prompt_df, person_to_category):\n",
    "    types = [\"Team-Team\", \"Team-Stat\", \"Stat-Stat\"]\n",
    "    person_to_type = {person: {t: [0, 0] for t in types} for person in person_to_category}\n",
    "    \n",
    "    for person, games in texts.items():\n",
    "        for game in games:\n",
    "            id = game.grid_number\n",
    "            prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "            if len(prompt_rows) != 1:\n",
    "                continue\n",
    "            prompts = prompt_rows.iloc[0][1:]\n",
    "    \n",
    "            matrix = game.matrix\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "                    tag = \"\"\n",
    "                    if category_is_team(part_one) and category_is_team(part_two):\n",
    "                        tag = \"Team-Team\"\n",
    "                    elif category_is_team(part_one) != category_is_team(part_two):\n",
    "                        tag = \"Team-Stat\"\n",
    "                    else:\n",
    "                        tag = \"Stat-Stat\"\n",
    "                    if matrix[i][j]:\n",
    "                        person_to_type[person][tag][0] += 1\n",
    "                    person_to_type[person][tag][1] += 1\n",
    "    return person_to_type\n",
    "\n",
    "def person_to_type_to_string(person_to_type):\n",
    "    result = \"\"\n",
    "    for person in person_to_type:\n",
    "        result += person + \"\\n\"\n",
    "        for tag in person_to_type[person]:\n",
    "            acc = person_to_type[person][tag][0] / person_to_type[person][tag][1]\n",
    "            line = f\"{tag}: {round(100 * acc)}% ({person_to_type[person][tag][1]})\"\n",
    "            result += line + \"\\n\"\n",
    "        result += \"\\n\"\n",
    "    return result\n",
    "\n",
    "def person_to_category_to_string(person_to_category, threshold=25):\n",
    "    result = \"\"\n",
    "    for person, value in person_to_category.items():\n",
    "        rankings = sorted([(cat, value[cat][0] / value[cat][1], value[cat][1]) for cat in value], key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        result += f\"====={person}=====\\n\" \n",
    "        count = 1\n",
    "        for i, (category, accuracy, total) in enumerate(rankings):\n",
    "            if total > threshold:\n",
    "                result += f\"{count}. {category} ({round(accuracy, 2)}) ({total})\\n\"\n",
    "                count += 1\n",
    "        result += \"\\n\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def analyze_easiest_teams(categories, person_to_category):\n",
    "    overall = []\n",
    "    for category in categories:\n",
    "        values = []\n",
    "        counts = []\n",
    "        for person in person_to_category:\n",
    "            values.append(person_to_category[person][category][0] / person_to_category[person][category][1])\n",
    "            counts.append(person_to_category[person][category][1])\n",
    "        if category_is_team(category):\n",
    "            overall.append((category, sum(values) / len(values)))\n",
    "\n",
    "    result = \"\"\n",
    "    result += \"Consensus Easiest Teams\\n\"\n",
    "    overall = sorted(overall, key=lambda x: x[1], reverse=True)\n",
    "    for i, (category, avg) in enumerate(overall):\n",
    "        result += f\"{(i + 1)}. {category} ({round(100 * avg)}%)\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def analyze_team_std_dev(categories, person_to_category):\n",
    "    overall = []\n",
    "    for category in categories:\n",
    "        values = []\n",
    "        counts = []\n",
    "        for person in person_to_category:\n",
    "            values.append(person_to_category[person][category][0] / person_to_category[person][category][1])\n",
    "            counts.append(person_to_category[person][category][1])\n",
    "        if category_is_team(category):\n",
    "            overall.append((category, np.std(values)))\n",
    "\n",
    "    result = \"\"\n",
    "    result += \"Biggest Team Standard Deviations\\n\"\n",
    "    overall = sorted(overall, key=lambda x: x[1], reverse=True)\n",
    "    for i, (category, avg) in enumerate(overall):\n",
    "        result += f\"{(i + 1)}. {category} ({round(100 * avg)}%)\\n\"\n",
    "    return result\n",
    "\n",
    "def analyze_best_person_by_team(categories, person_to_category):\n",
    "    overall = []\n",
    "    for category in filter(category_is_team, categories):\n",
    "    \n",
    "        max_acc = 0\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "    \n",
    "        max_people = []\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if abs(acc - max_acc) < 0.0001:\n",
    "                max_people.append(person)\n",
    "        \n",
    "        overall.append((category, \", \".join(max_people)))\n",
    "\n",
    "    result = \"\"\n",
    "    result += \"Best Person for Each Team\\n\"\n",
    "    for category, people in sorted(overall, key=lambda x: x[0]):\n",
    "        if len(category) > 15:\n",
    "            result += category + \"\\t\\t\"\n",
    "        else:\n",
    "            result += category + \"\\t\\t\\t\"\n",
    "        result += people\n",
    "        result += \"\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def analyze_worst_person_by_team(categories, person_to_category):\n",
    "    overall = []\n",
    "    for category in filter(category_is_team, categories):\n",
    "    \n",
    "        min_acc = 101\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if acc < min_acc:\n",
    "                min_acc = acc\n",
    "    \n",
    "        min_people = []\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if abs(acc - min_acc) < 0.0001:\n",
    "                min_people.append(person)\n",
    "        \n",
    "        overall.append((category, \", \".join(min_people)))\n",
    "    \n",
    "    result = \"\"\n",
    "    result += \"Worst Person for Each Team\\n\"\n",
    "    for category, people in sorted(overall, key=lambda x: x[0]):\n",
    "        if len(category) > 15:\n",
    "            result += category + \"\\t\\t\"\n",
    "        else:\n",
    "            result += category + \"\\t\\t\\t\"\n",
    "        result += people\n",
    "        result += \"\\n\"\n",
    "\n",
    "    return result    \n",
    "\n",
    "def analyze_best_person_by_category(categories, person_to_category, categories_clearing_threshold):\n",
    "    overall = []\n",
    "    for category in filter(lambda x: not category_is_team(x) and x in categories_clearing_threshold, categories):\n",
    "    \n",
    "        max_acc = 0\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "    \n",
    "        max_people = []\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if abs(acc - max_acc) < 0.0001:\n",
    "                max_people.append(person)\n",
    "        \n",
    "        overall.append((category, \", \".join(max_people)))\n",
    "\n",
    "    result = \"\"\n",
    "    result += \"Best Person for Each Category\\n\"\n",
    "    for category, people in sorted(overall, key=lambda x: x[0]):\n",
    "        if len(category) > 15:\n",
    "            result += category + \"\\t\\t\"\n",
    "        else:\n",
    "            result += category + \"\\t\\t\\t\"\n",
    "        result += people\n",
    "        result += \"\\n\"\n",
    "\n",
    "    return result    \n",
    "\n",
    "def analyze_worst_person_by_category(categories, person_to_category, categories_clearing_threshold):\n",
    "    overall = []\n",
    "    for category in filter(lambda x: not category_is_team(x) and x in categories_clearing_threshold, categories):\n",
    "    \n",
    "        min_acc = 101\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if acc < min_acc:\n",
    "                min_acc = acc\n",
    "    \n",
    "        min_people = []\n",
    "        for person in person_to_category:\n",
    "            acc = person_to_category[person][category][0] / person_to_category[person][category][1]\n",
    "            if abs(acc - min_acc) < 0.0001:\n",
    "                min_people.append(person)\n",
    "        \n",
    "        overall.append((category, \", \".join(min_people)))\n",
    "    \n",
    "    result = \"\"\n",
    "    result += \"Worst Person for Each Category\\n\"\n",
    "    for category, people in sorted(overall, key=lambda x: x[0]):\n",
    "        if len(category) > 15:\n",
    "            result += category + \"\\t\\t\"\n",
    "        else:\n",
    "            result += category + \"\\t\\t\\t\"\n",
    "        result += people\n",
    "        result += \"\\n\"\n",
    "\n",
    "    return result   \n",
    "\n",
    "def analyze_person_prompt_performance(categories, person_to_category, categories_clearing_threshold, direction, category_type):\n",
    "    if category_type == \"Team\" and direction == \"Best\":\n",
    "        return analyze_best_person_by_team(categories, person_to_category)\n",
    "    elif category_type == \"Team\" and direction == \"Worst\":\n",
    "        return analyze_worst_person_by_team(categories, person_to_category)\n",
    "    elif category_type == \"Category\" and direction == \"Best\":\n",
    "        return analyze_best_person_by_category(categories, person_to_category, categories_clearing_threshold)\n",
    "    elif category_type == \"Category\" and direction == \"Worst\":\n",
    "        return analyze_worst_person_by_category(categories, person_to_category, categories_clearing_threshold)\n",
    "    return \"\"\n",
    "\n",
    "def analyze_hardest_teams(texts, prompt_df):\n",
    "    result = StringIO()\n",
    "    \n",
    "    hardest_teams = {}\n",
    "    \n",
    "    for person, games in texts.items():\n",
    "        hardest_teams[person] = {team: [0, 0] for team in TEAM_LIST}\n",
    "        for game in games:\n",
    "            id = game.grid_number\n",
    "            prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "            if len(prompt_rows) != 1:\n",
    "                continue\n",
    "            prompts = prompt_rows.iloc[0][1:]\n",
    "    \n",
    "            matrix = game.matrix\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "                    tag = \"\"\n",
    "                    if category_is_team(part_one) and category_is_team(part_two):\n",
    "                        team_one = get_team_from_category(part_one)\n",
    "                        team_two = get_team_from_category(part_two)\n",
    "                        if matrix[i][j]:\n",
    "                            hardest_teams[person][team_one][0] += 1\n",
    "                            hardest_teams[person][team_two][0] += 1\n",
    "                        hardest_teams[person][team_one][1] += 1\n",
    "                        hardest_teams[person][team_two][1] += 1\n",
    "    \n",
    "    print(\"Hardest Team-Team Intersections for Each Person\", \"\\n\\n\", file=result)\n",
    "    for person in hardest_teams:\n",
    "        print(f\"====={person}=====\", file=result)\n",
    "        for i, (team, res) in enumerate(sorted(hardest_teams[person].items(), key = lambda x: x[1][0] / x[1][1], reverse=True)):\n",
    "            print(f\"{i + 1}. {team} ({round(100 * res[0] / res[1])}%)\", file=result)\n",
    "        print(\"\\n\\n\\n\", file=result)\n",
    "    \n",
    "    consensus_intersection_difficulty = {}\n",
    "    for team in TEAM_LIST:\n",
    "        right = 0\n",
    "        total = 0\n",
    "        for person in hardest_teams:\n",
    "            res = hardest_teams[person][team]\n",
    "            right += res[0]\n",
    "            total += res[1]\n",
    "        consensus_intersection_difficulty[team] = right / total\n",
    "        \n",
    "    print(\"=====Consensus=====\", file=result)\n",
    "    for i, (team, pct) in enumerate(sorted(consensus_intersection_difficulty.items(), key=lambda x: x[1], reverse=True)):\n",
    "        print(f\"{i + 1}. {team} ({round(100 * pct)}%)\", file=result)\n",
    "\n",
    "    # Get the full output as a string\n",
    "    result_string = result.getvalue()\n",
    "    result.close()  # Close the StringIO object\n",
    "    return result_string\n",
    "\n",
    "def analyze_hardest_team_stats(texts, prompt_df):\n",
    "    result = StringIO()\n",
    "\n",
    "    hardest_team_stats = {}\n",
    "    \n",
    "    for person, games in texts.items():\n",
    "        hardest_team_stats[person] = {team: [0, 0] for team in TEAM_LIST}\n",
    "        for game in games:\n",
    "            id = game.grid_number\n",
    "            prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "            if len(prompt_rows) != 1:\n",
    "                continue\n",
    "            prompts = prompt_rows.iloc[0][1:]\n",
    "    \n",
    "            matrix = game.matrix\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "                    tag = \"\"\n",
    "                    if category_is_team(part_one) and not category_is_team(part_two):\n",
    "                        team_one = get_team_from_category(part_one)\n",
    "                        if matrix[i][j]:\n",
    "                            hardest_team_stats[person][team_one][0] += 1\n",
    "                        hardest_team_stats[person][team_one][1] += 1\n",
    "                    elif not category_is_team(part_one) and category_is_team(part_two):\n",
    "                        team_two = get_team_from_category(part_two)\n",
    "                        if matrix[i][j]:\n",
    "                            hardest_team_stats[person][team_two][0] += 1\n",
    "                        hardest_team_stats[person][team_two][1] += 1\n",
    "    \n",
    "    print(\"Hardest Team-Stats Intersections for Each Person\", \"\\n\\n\", file=result)\n",
    "    for person in hardest_team_stats:\n",
    "        print(f\"====={person}=====\", file=result)\n",
    "        for i, (team, res) in enumerate(sorted(hardest_team_stats[person].items(), key = lambda x: x[1][0] / x[1][1], reverse=True)):\n",
    "            print(f\"{i + 1}. {team} ({round(100 * res[0] / res[1])}%)\", file=result)\n",
    "        print(\"\\n\\n\\n\", file=result)\n",
    "    \n",
    "    consensus_intersection_difficulty = {}\n",
    "    for team in TEAM_LIST:\n",
    "        right = 0\n",
    "        total = 0\n",
    "        for person in hardest_team_stats:\n",
    "            res = hardest_team_stats[person][team]\n",
    "            right += res[0]\n",
    "            total += res[1]\n",
    "        consensus_intersection_difficulty[team] = right / total\n",
    "        \n",
    "    print(\"=====Consensus=====\", file=result)\n",
    "    for i, (team, pct) in enumerate(sorted(consensus_intersection_difficulty.items(), key=lambda x: x[1], reverse=True)):\n",
    "        print(f\"{i + 1}. {team} ({round(100 * pct)}%)\", file=result)\n",
    "\n",
    "    # Get the full output as a string\n",
    "    result_string = result.getvalue()\n",
    "    result.close()  # Close the StringIO object\n",
    "    return result_string\n",
    "\n",
    "def compute_most_common_exact_intersections(texts, prompt_df, name):\n",
    "    most_common_exact_intersections = {}\n",
    "    \n",
    "    for game in texts[name]:\n",
    "        id = game.grid_number\n",
    "        prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "        if len(prompt_rows) != 1:\n",
    "            continue\n",
    "        prompts = prompt_rows.iloc[0][1:]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "                key = \" + \".join(sorted([part_one, part_two]))\n",
    "                if key not in most_common_exact_intersections:\n",
    "                    most_common_exact_intersections[key] = 0\n",
    "                most_common_exact_intersections[key] += 1\n",
    "    return most_common_exact_intersections\n",
    "\n",
    "def analyze_most_common_exact_intersections(texts, prompt_df, name):\n",
    "    result = StringIO()\n",
    "    \n",
    "    most_common_exact_intersections = compute_most_common_exact_intersections(texts, prompt_df, name)\n",
    "\n",
    "    print(\"Most Common Exact Intersections for {}\".format(name), file=result)\n",
    "    for i, (combo, count) in enumerate(sorted(most_common_exact_intersections.items(), key = lambda x: x[1], reverse=True)):\n",
    "        if count >= 5:\n",
    "            print(f\"{i + 1}. {combo} ({count})\", file=result)\n",
    "\n",
    "    # Get the full output as a string\n",
    "    result_string = result.getvalue()\n",
    "    result.close()  # Close the StringIO object\n",
    "    return result_string\n",
    "\n",
    "def analyze_empty_team_team_intersections(texts, prompt_df, name, categories):\n",
    "    result = StringIO()\n",
    "\n",
    "    most_common_exact_intersections = compute_most_common_exact_intersections(texts, prompt_df, name)\n",
    "        \n",
    "    team_to_full_names = {}\n",
    "    full_names_to_team = {}\n",
    "    for team in TEAM_LIST:\n",
    "        for category in categories:\n",
    "            if team in category:\n",
    "                team_to_full_names[team] = category\n",
    "                full_names_to_team[category] = team\n",
    "                \n",
    "    \n",
    "    missing = 0\n",
    "    present = 0\n",
    "    missing_maps = {}\n",
    "    print(\"Empty Team-Team Intersections for {}\".format(name), file=result)\n",
    "    for i, team in enumerate(sorted(TEAM_LIST)):\n",
    "        for other in sorted(TEAM_LIST)[i + 1:]:\n",
    "            key = \" + \".join([team_to_full_names[team], team_to_full_names[other]])\n",
    "            other_key =  \" + \".join([team_to_full_names[other], team_to_full_names[team]])\n",
    "            if key not in most_common_exact_intersections and other_key not in most_common_exact_intersections:\n",
    "                print(key, file=result)\n",
    "                missing += 1\n",
    "                if team not in missing_maps:\n",
    "                    missing_maps[team] = 0\n",
    "                if other not in missing_maps:\n",
    "                    missing_maps[other] = 0\n",
    "                missing_maps[team] += 1\n",
    "                missing_maps[other] += 1\n",
    "            else:\n",
    "                present += 1    \n",
    "    \n",
    "    print(\"\\n\\n\\n\\nTotal Missing for {}\".format(name), file=result)\n",
    "    for i, (team, count) in enumerate(sorted(missing_maps.items(), key=lambda x: x[1], reverse=True)):\n",
    "        if count > 0:\n",
    "            print(f\"{i + 1}. {team} ({count})\", file=result)\n",
    "\n",
    "    # Get the full output as a string\n",
    "    result_string = result.getvalue()\n",
    "    result.close()  # Close the StringIO object\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbc6a968-1616-40f2-9917-3e0ecf9af52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute\n",
    "prompt_df = read_prompt_data(INPUT_PROMPT_DATA_PATH)\n",
    "categories = build_category_structure(prompt_df)\n",
    "person_to_category = build_person_category_structure(texts, prompt_df, categories)\n",
    "categories_clearing_threshold = get_category_clearing_threshold(categories, person_to_category)\n",
    "person_to_type = get_person_to_type(texts, prompt_df, person_to_category)\n",
    "\n",
    "x = person_to_type_to_string(person_to_type)\n",
    "y = person_to_category_to_string(person_to_category)\n",
    "z = analyze_easiest_teams(categories, person_to_category)\n",
    "a = analyze_team_std_dev(categories, person_to_category)\n",
    "b = analyze_person_prompt_performance(categories, person_to_category, categories_clearing_threshold, \"Best\", \"Team\")\n",
    "c = analyze_person_prompt_performance(categories, person_to_category, categories_clearing_threshold, \"Worst\", \"Team\")\n",
    "d = analyze_person_prompt_performance(categories, person_to_category, categories_clearing_threshold, \"Best\", \"Category\")\n",
    "e = analyze_person_prompt_performance(categories, person_to_category, categories_clearing_threshold, \"Worst\", \"Category\")\n",
    "f = analyze_hardest_teams(texts, prompt_df)\n",
    "g = analyze_hardest_team_stats(texts, prompt_df)\n",
    "h = analyze_most_common_exact_intersections(texts, prompt_df, \"Keith\")\n",
    "i = analyze_empty_team_team_intersections(texts, prompt_df, \"Keith\", categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "496fe641-bedc-4423-b2cd-94c419b133e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:145: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:172: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:184: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:196: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:208: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:72: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file './immaculate_grid_report.pdf' has been created with a cover page, table of contents, and all graphs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:135: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/56802756.py:279: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()  # Close the figure\n",
      "/var/folders/8d/vq0d3j092zj6m8r71b7crw5r0000gn/T/ipykernel_1623/3330395297.py:22: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "## To do:\n",
    "# I have a way to make text string outputs from functions when executed\n",
    "# I have a way to make text show up on PDF with the text to display as a parameter\n",
    "# Just need to pipe the text from the function outputs into the pdf displayer (thinking we use func(*args))\n",
    "\n",
    "def make_generic_text_page(func, args, page_title):\n",
    "\n",
    "    output = func(*args)\n",
    "    \n",
    "    # Create a new figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(8, 11))\n",
    "    \n",
    "    # Hide the axes for a clean canvas\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.text(0.5, 0.95, page_title, fontsize=24, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Add text to the plot\n",
    "    plt.text(0.5, 0.8, output, fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "def create_pdf_with_graphs_cover_and_toc(texts, COLOR_MAP, analysis_df, smoothed_metrics_df, reversed_dict, pdf_filename):\n",
    "    \"\"\"\n",
    "    Creates a PDF booklet with a cover page, table of contents, various graphs, \n",
    "    and a summary table of best and worst scores based on the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: Data used for creating graphs.\n",
    "    - COLOR_MAP: Color mapping for the graphs.\n",
    "    - analysis_df: DataFrame containing analysis metrics.\n",
    "    - smoothed_metrics_df: DataFrame for smoothed metrics over time.\n",
    "    - reversed_dict: Dictionary for win rates.\n",
    "    - pdf_filename: Name of the output PDF file.\n",
    "    \"\"\"\n",
    "    # Use a non-interactive backend to prevent plots from rendering to the screen\n",
    "    plt.switch_backend('Agg')\n",
    "\n",
    "    # Get today's date in a readable format\n",
    "    today_date = datetime.now().strftime('%B %d, %Y')\n",
    "\n",
    "    # List of graph-making functions with their respective arguments and titles\n",
    "    graph_functions = [\n",
    "        (make_fig_1, (texts, COLOR_MAP), \"Number of Immaculates\"),\n",
    "        (make_fig_2, (texts, COLOR_MAP), \"Correctness Distribution\"),\n",
    "        (make_fig_3, (analysis_df, COLOR_MAP), \"Average Correct\"),\n",
    "        (make_fig_4, (analysis_df, COLOR_MAP), \"Average Score\"),\n",
    "        (make_fig_5, (analysis_df, COLOR_MAP), \"Average Rarity of Correct Square\"),\n",
    "        (make_fig_6, (smoothed_metrics_df,), \"Smoothed Scores Over Time\"),\n",
    "        (make_fig_7, (smoothed_metrics_df,), \"Smoothed Correct Over Time\"),\n",
    "        (make_fig_8, (smoothed_metrics_df,), \"Smoothed Avg Score of Correct Over Time\"),\n",
    "        (make_fig_9, (reversed_dict,), \"Win Rates\"),\n",
    "        (make_fig_10, (texts, ), 'Best and Worst Scores (All Time)'),\n",
    "        (make_fig_11, (texts, ), 'Best and Worst Scores (Last 30 Days)'),\n",
    "        (make_generic_text_page, (person_to_type_to_string, (person_to_type, ), 'Type Performance Overview'), 'Type Performance Overview')\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Create a PDF file with multiple pages\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            # Create the cover page\n",
    "            plt.figure(figsize=(8.5, 11))  # Set the page size to standard A4 or letter\n",
    "            plt.text(0.5, 0.7, 'Immaculate Grid Analysis Results', fontsize=24, ha='center', va='center', fontweight='bold')\n",
    "            plt.text(0.5, 0.6, f'Date of Analysis: {today_date}', fontsize=16, ha='center', va='center')\n",
    "            plt.axis('off')  # Hide axes for the cover page\n",
    "            pdf.savefig()  # Save the cover page to the PDF\n",
    "            plt.close()  # Close the figure for the cover page\n",
    "\n",
    "            # Create the Table of Contents page\n",
    "            plt.figure(figsize=(8.5, 11))\n",
    "            plt.text(0.5, 0.9, 'Table of Contents', fontsize=20, ha='center', va='top', fontweight='bold')\n",
    "\n",
    "            # Add the list of graphs to the Table of Contents\n",
    "            toc_item_y_position = 0.8\n",
    "            for i, (_, _, title) in enumerate(graph_functions, start=1):\n",
    "                plt.text(0.1, toc_item_y_position, f'{i}. {title}', fontsize=12, ha='left', va='top')\n",
    "                toc_item_y_position -= 0.05  # Adjust the position for the next line\n",
    "         \n",
    "            plt.axis('off')  # Hide axes for the Table of Contents page\n",
    "            pdf.savefig()  # Save the Table of Contents page to the PDF\n",
    "            plt.close()  # Close the figure for the Table of Contents page\n",
    "\n",
    "            # Add each graph to a new page in the PDF\n",
    "            for func, args, _ in graph_functions:\n",
    "                plt.figure()\n",
    "                func(*args)  # Call the graph-making function with its arguments\n",
    "                pdf.savefig()  # Save the current figure to the PDF\n",
    "                plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(f\"PDF file '{pdf_filename}' has been created with a cover page, table of contents, and all graphs.\")\n",
    "\n",
    "# # Calculate the smoothed metrics from the DataFrame\n",
    "smoothness = 28\n",
    "smoothed_metrics_df = calculate_smoothed_metrics(analysis_df, smoothness)\n",
    "create_pdf_with_graphs_cover_and_toc(texts, COLOR_MAP, analysis_df, smoothed_metrics_df, reversed_dict, pdf_filename=PDF_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76ce37fc-d665-452b-a3e0-991c4ac525e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "most_common_exact_intersections = {}\n",
    "\n",
    "for game in texts[\"Rachel\"]:\n",
    "    id = game.grid_number\n",
    "    prompt_rows = prompt_df[prompt_df[\"game_id\"] == id]\n",
    "    if len(prompt_rows) != 1:\n",
    "        continue\n",
    "    prompts = prompt_rows.iloc[0][1:]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            part_one, part_two = get_categories_from_prompt(prompts[f\"{i}{j}\"])\n",
    "            teams = [get_team_from_category(part_one), get_team_from_category(part_two)]\n",
    "            if \"Guardians\" in teams and \"Athletics\" in teams:\n",
    "                print(game.matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04e79274-8c0b-4efb-aea5-2b88e51825a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './immaculate_grid_names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./immaculate_grid_names.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     names \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './immaculate_grid_names.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"./immaculate_grid_names.txt\") as f:\n",
    "    names = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff89790-0e83-4fd5-9846-c6c928359d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_count = {}\n",
    "for name in names:\n",
    "    name_to_count[name] = name_to_count.get(name, 0) + 1\n",
    "sorted_names = sorted(name_to_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7e15e-7606-4dc9-a5f0-e418937a2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, count) in enumerate(sorted_names):\n",
    "    print(f\"{i + 1}. {name} ({count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc03c88-30d0-4a76-87ed-99198f994ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variants(existing, all):\n",
    "    if len(existing) == 9:\n",
    "        all.add(\"\".join([str(x) for x in existing]))\n",
    "        return\n",
    "    create_variants(deepcopy(existing) + [0], all)\n",
    "    create_variants(deepcopy(existing) + [1], all)\n",
    "    return\n",
    "combos = set()\n",
    "create_variants([], combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286968f-d504-4551-8cb0-6315353ace71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current grid number by selecting the max grid number from all ImmaculateGridResult objects for each person\n",
    "current_grid_number = max(\n",
    "    [\n",
    "        max(\n",
    "            [result.grid_number for result in texts[person] if result.grid_number is not None]\n",
    "        )\n",
    "        for person in texts\n",
    "    ]\n",
    ")\n",
    "\n",
    "# copied_variants = deepcopy(combos)\n",
    "# full_combos = set()\n",
    "# dates = []\n",
    "# for grid_num in range(current_grid_number):\n",
    "#     for person in texts:\n",
    "#         if grid_num not in texts[person]:\n",
    "#             continue\n",
    "#         obj = texts[person][grid_num]\n",
    "#         current = \"\"\n",
    "#         for text_row in obj.text.split(\"\\n\"):\n",
    "#             for char in text_row:\n",
    "#                 if ord(char) == 11036: #\"\":\n",
    "#                     current += \"0\"\n",
    "#                 elif ord(char) == 129001: #\"\":\n",
    "#                     current += \"1\"\n",
    "#         assert len(current) == 9\n",
    "#         full_combos.add(current)\n",
    "#         if current in copied_variants:\n",
    "#             copied_variants.remove(current)\n",
    "#             dates.append((obj.date, grid_num, person, current, obj.text))\n",
    "\n",
    "# sorted_dates = sorted(dates, key = lambda x: x[1], reverse=True)\n",
    "# for date, _, person, combo, text in sorted_dates[:10]:\n",
    "#     print(person, date, \"\\n\", text, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936786f6-1700-4434-900e-3842c6130821",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[\"Keith\"][152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a563d2-a34d-4b55-b120-c3f5a911199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
